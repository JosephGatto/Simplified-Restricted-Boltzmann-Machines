{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf #Deep learning library\n",
    "import numpy as np #Matrix algebra library\n",
    "from tensorflow.examples.tutorials.mnist import input_data #Import training data\n",
    "import pandas as pd #Database management library\n",
    "import matplotlib.pyplot as plt #Visualization library \n",
    "%matplotlib inline  \n",
    "from sklearn.svm import LinearSVC #For linear image classification\n",
    "from sklearn.model_selection import GridSearchCV #For hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST Dataset & Prepare Data for RBM \n",
    "\n",
    "We will be building this RBM in Tensorflow and testing it on the MNIST dataset. We will not need one_hot encoding as we will be using our RBM to extract important features from our image data to be used in a linear classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=False) \n",
    "trX, trY, teX, teY = mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restricted Boltzmann Machine\n",
    "\n",
    "### Who is this explanation for?  \n",
    "\n",
    "At the risk of sacrificing some important information, I will be discussing this with you as I would a close friend who knew a little about deep learning. Sometimes, I think its nice to get a general overview of how something works before diving into the specifics. If you are someone like me who was taking Geoffrey Hinton's Coursera course and had no idea what he was talking about when he got into Restricted Boltzmann Machines...keep reading! \n",
    "\n",
    "### How will we be utilizing this RBM?\n",
    "\n",
    "When my research team began developing a new method of feature engineering for image classification, they were very interested to see how it compared to the classic RBM. Naturally, I was given the tedious task of building one and producing benchmarks for a variety of datasets. The goal of this tutorial is to utilize the RBM to extract important features from an image, and then use those features in a simple linear classifier. We want to take image data that is NOT linearly seperable and attempt to make it... more linearly seperable by using the RBM as a feature engineering tool. This is only one of many use cases for the RBM.\n",
    "\n",
    "\n",
    "### How do we do that? \n",
    "\n",
    "The RBM is an unsupervised model, meaning, there is no known output we are mapping our data to. We have our input layer (which is just the pixels of our image) and a hidden layer which will, over time, be learning what the most important features are. Now I know I said I am assuming you know a little about deep learning, but before we get into how RBMs work we should probably set a few definitions. \n",
    "\n",
    "Back propagation: This is a tool that updates the weights of our neural network in a direction that minimizes our error. Over time, these weights will allow our neural network (or RBM in this case) to learn whatever it is we are asking it to learn. \n",
    "\n",
    "Dense layer: In our case, this simply means that every neuron in the input layer is connected to every neuron of the hidden layer. This does not mean that neurons in the input layer are connected to other neurons in the input layer. Input layer neurons are only connected to neurons in the hidden layer. \n",
    "\n",
    "Input Layer: The layer of the RBM that contains the original image data. \n",
    "\n",
    "Hidden Layer: The layer of the RBM that is extracting the key features from the image. The goal of the hidden layer is to eventually reconstruct what it sees in the input layer without using all of the original pixels. It only uses features it deems most important. Lets talk about how this happens.\n",
    "\n",
    "### Inside of the RBM\n",
    "\n",
    "So this is what's going down. Lets take one image from the MNIST dataset and discuss what happens as it goes through a restricted boltzmann machine. MNIST is full of 28x28 images of hand-written digits. First, we take our 28x28 image and flatten it to a 1D array of length 784 (because 28*28 = 784). Now, if you are picturing an artificial neural network in your head, these are all of the neurons of the input layer. Now we need to decide how many neurons we wish to have in the hidden layer. \n",
    "\n",
    "The hidden layer is a magical place. A place where you can reconstruct the original image with less neurons than you started with. What?!??!? Thats right! The goal of the hidden layer is to extract the most important features from the original image and then use those features to reconstruct the original image! So earlier I asked the question: How many neurons should we have in the hidden layer? The answer: I dont know. As you may know by now, machine learning is an iterative process and finding the magic number will take some time. Often, people will reduce the number of neurons in the hidden layer, for example, going from 784 neurons to 650 neurons. Thus, the RBM now has to work hard to capture the essence of the original image in only 650 neurons. How does it do this? Well, lets stop and recap what we are currently looking at. \n",
    "\n",
    "1) Image gets flattened into a 784 length array and prepared for the input layer of the RBM. \n",
    "\n",
    "2) We decided that the hidden layer size is 650. So now we will randomly initalize our weights that are connecting each neuron from the input layer to each neuron of the hidden layer. \n",
    "\n",
    "These weights connecting the two layers are what is going to allow the RBM to model the original image with a smaller amount of neurons. Using back propagation, the weights will, over time, learn how to help the hidden layer represent the original image with a smaller number of pixels. \n",
    "\n",
    "### How do we know if it's working? \n",
    "\n",
    "So we have inputted our image, created our hidden layer, and our weights are randomly initalized and ready to go. Now what? Well, the RBM is going to attempt to reconstruct the original 28*28 image with only 650 neurons. On its first go around, it will probably do pretty bad considering we gave the model random weights to work with. We can check how bad it did by measuring the difference between the hidden layer reconstruction and the original image. After that, the RBM will go through another iteration, this time updating the weights to help the hidden layer reconstruction better represent the original image, and again, we check to see the difference between the original image and the reconstruction. This proceess goes on and on until we find that our error is low and the reconstructed image closely resembles the original image.\n",
    "\n",
    "### Linear classification time! \n",
    "\n",
    "Now that our hidden layer is able to reconstruct the original image extremely well with only 650 neurons, lets see how these 650 neurons perform in a linear classifier. With no feature engineering, meaning, just using our 784 original neurons as our training data, I was able to get 91% accuracy using a LinearSVC. After running the RBM, extracting the 650 neurons containing our high quality features from the hidden layer, and using those features as training data, I got 97% accuracy! I could probably get better results if I experimented with different numbers of neurons in my hidden layer. Let me know if you beat 97%!\n",
    "\n",
    "### Now its your turn!\n",
    "\n",
    "Follow along with the code below. I have heavily commented it for you and if you have any questions feel free to email me. \n",
    "\n",
    "### Contrastive Divergence\n",
    "\n",
    "If you are going through Geoffrey Hinton's course and learned about Contrastive Divergence, I have a tensorflow implementation for you. Feel free to try it out. I personally have never come across a time where CD-n for n > 1 performed better than n = 1 but after hearing so much about it I had to try. \n",
    "\n",
    "For those wondering, all Contrastive Divergence means is, instead of the RBM iterating over and over again going from original input to reconstruction > original input to better reconstruction and so on... now you are using the reconstruction as the input layer in the second iteration... so original input to reconstruction to reconstruction of the reconstruction and so on. It's not very popular but can be fun to play with! \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RBM(object):\n",
    "    def __init__(self, input_size, output_size, learning_rate, batch_size):\n",
    "        self.input_size = input_size #Size of the input layer\n",
    "        self.output_size = output_size #Size of the hidden layer\n",
    "        self.epochs = 2 #How many times we will update the weights \n",
    "        self.learning_rate = learning_rate #How big of a weight update we will perform \n",
    "        self.batch_size = batch_size #How many images will we \"feature engineer\" at at time \n",
    "        self.new_input_layer = None #Initalize new input layer variable for k-step contrastive divergence \n",
    "        self.new_hidden_layer = None\n",
    "        self.new_test_hidden_layer = None\n",
    "        \n",
    "        #Here we initialize the weights and biases of our RBM\n",
    "        #If you are wondering, the 0 is the mean of the distribution we are getting our random weights from. \n",
    "        #The .01 is the standard deviation.\n",
    "        self.w = np.random.normal(0,.01,[input_size,output_size]) #weights\n",
    "        self.hb = np.random.normal(0,.01,[output_size]) #hidden layer bias\n",
    "        self.vb = np.random.normal(0,.01,[input_size]) #input layer bias (sometimes called visible layer)\n",
    "        \n",
    "        \n",
    "        #Calculates the sigmoid probabilities of input * weights + bias\n",
    "        #Here we multiply the input layer by the weights and add the bias\n",
    "        #This is the phase that creates the hidden layer\n",
    "    def prob_h_given_v(self, visible, w, hb):\n",
    "        return tf.nn.sigmoid(tf.matmul(visible, w) + hb)\n",
    "        \n",
    "        #Calculates the sigmoid probabilities of input * weights + bias\n",
    "        #Here we multiply the hidden layer by the weights and add the input layer bias\n",
    "        #This is the reconstruction phase that recreates the original image from the hidden layer\n",
    "    def prob_v_given_h(self, hidden, w, vb):\n",
    "        return tf.nn.sigmoid(tf.matmul(hidden, tf.transpose(w)) + vb)\n",
    "    \n",
    "    #Returns new layer binary values\n",
    "    #This function returns a 0 or 1 based on the sign of the probabilities passed to it\n",
    "    #Our RBM will be utilizing binary features to represent the images\n",
    "    #This function just converts the features we have learned into a binary representation \n",
    "    def sample_prob(self, probs):\n",
    "        return tf.nn.relu(tf.sign(probs - tf.random_uniform(tf.shape(probs))))\n",
    "    \n",
    "    def train(self, X, teX):\n",
    "        #Initalize placeholder values for graph\n",
    "        #If this looks strange to you, then you have not used Tensorflow before\n",
    "        _w = tf.placeholder(tf.float32, shape = [self.input_size, self.output_size])\n",
    "        _vb = tf.placeholder(tf.float32, shape = [self.input_size])\n",
    "        _hb = tf.placeholder(tf.float32, shape = [self.output_size])\n",
    "        \n",
    "        \n",
    "        #initalize previous variables\n",
    "        #we will be saving the weights of the previous and current iterations\n",
    "        pre_w = np.random.normal(0,.01, size = [self.input_size,self.output_size])\n",
    "        pre_vb = np.random.normal(0, .01, size = [self.input_size])\n",
    "        pre_hb = np.random.normal(0, .01, size = [self.output_size])\n",
    "        \n",
    "        #initalize current variables\n",
    "        #we will be saving the weights of the previous and current iterations\n",
    "        cur_w = np.random.normal(0, .01, size = [self.input_size,self.output_size])\n",
    "        cur_vb = np.random.normal(0, .01, size = [self.input_size])\n",
    "        cur_hb = np.random.normal(0, .01, size = [self.output_size])\n",
    "               \n",
    "        #Plaecholder variable for input layer\n",
    "        v0 = tf.placeholder(tf.float32, shape = [None, self.input_size])\n",
    "        \n",
    "        #pass probabilities of input * w + b into sample prob to get binary values of hidden layer\n",
    "        h0 = self.sample_prob(self.prob_h_given_v(v0, _w, _hb ))\n",
    "        \n",
    "        #pass probabilities of new hidden unit * w + b into sample prob to get new reconstruction\n",
    "        v1 = self.sample_prob(self.prob_v_given_h(h0, _w, _vb))\n",
    "        \n",
    "        #Just get the probailities of the next hidden layer. We wont need the binary values. \n",
    "        #The probabilities here help calculate the gradients during back prop \n",
    "        h1 = self.prob_h_given_v(v1, _w, _hb)\n",
    "        \n",
    "        \n",
    "        #Contrastive Divergence\n",
    "        positive_grad = tf.matmul(tf.transpose(v0), h0) #input' * hidden0\n",
    "        negative_grad = tf.matmul(tf.transpose(v1), h1) #reconstruction' * hidden1\n",
    "        #(pos_grad - neg_grad) / total number of input samples \n",
    "        CD = (positive_grad - negative_grad) / tf.to_float(tf.shape(v0)[0]) \n",
    "        \n",
    "        #This is just the definition of contrastive divergence \n",
    "        update_w = _w + self.learning_rate * CD\n",
    "        update_vb = _vb + tf.reduce_mean(v0 - v1, 0)\n",
    "        update_hb = _hb + tf.reduce_mean(h0 - h1, 0)\n",
    "        \n",
    "        #MSE - This is our error function\n",
    "        err = tf.reduce_mean(tf.square(v0 - v1))\n",
    "        \n",
    "        #Will hold new visible layer.\n",
    "        errors = []\n",
    "        hidden_units = []\n",
    "        reconstruction = []\n",
    "        \n",
    "        test_hidden_units = []\n",
    "        test_reconstruction=[]\n",
    "        \n",
    "        \n",
    "        #The next four lines of code intitalize our Tensorflow graph and create mini batches\n",
    "        #The mini batch code is from cognitive class. I love the way they did this. Just giving credit! \n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for epoch in range(self.epochs):\n",
    "                for start, end in zip(range(0, len(X), self.batch_size), range(self.batch_size, len(X), self.batch_size)):\n",
    "                    batch = X[start:end] #Mini batch of images taken from training data\n",
    "                    \n",
    "                    #Feed in batch, previous weights/bias, update weights and store them in current weights\n",
    "                    cur_w = sess.run(update_w, feed_dict = {v0:batch, _w:pre_w , _vb:pre_vb, _hb:pre_hb})\n",
    "                    cur_hb = sess.run(update_hb, feed_dict = {v0:batch, _w:pre_w , _vb:pre_vb, _hb:pre_hb})\n",
    "                    cur_vb = sess.run(update_vb, feed_dict = {v0:batch, _w:pre_w , _vb:pre_vb, _hb:pre_hb})\n",
    "                    \n",
    "                    #Save weights \n",
    "                    pre_w = cur_w\n",
    "                    pre_hb = cur_hb\n",
    "                    pre_vb = cur_vb\n",
    "                \n",
    "                #At the end of each iteration, the reconstructed images are stored and the error is outputted \n",
    "                reconstruction.append(sess.run(v1, feed_dict={v0: X, _w: cur_w, _vb: cur_vb, _hb: cur_hb}))        \n",
    "                print('Learning Rate: {}:  Batch Size: {}:  Hidden Layers: {}: Epoch: {}: Error: {}:'.format(self.learning_rate, self.batch_size, \n",
    "                                                                                                             self.output_size, (epoch+1),\n",
    "                                                                                                            sess.run(err, feed_dict={v0: X, _w: cur_w, _vb: cur_vb, _hb: cur_hb})))\n",
    "            \n",
    "            #Store final reconstruction in RBM object\n",
    "            self.new_input_layer = reconstruction[-1]\n",
    "            \n",
    "            #Store weights in RBM object\n",
    "            self.w = pre_w\n",
    "            self.hb = pre_hb\n",
    "            self.vb = pre_vb\n",
    "    \n",
    "    #This is used for Contrastive Divergence.\n",
    "    #This function makes the reconstruction your new input layer. \n",
    "    def rbm_output(self, X):\n",
    "        input_x = tf.constant(X)\n",
    "        _w = tf.constant(self.w)\n",
    "        _hb = tf.constant(self.hb)\n",
    "        _vb = tf.constant(self.vb)\n",
    "        \n",
    "        out = tf.nn.sigmoid(tf.matmul(input_x, _w) + _hb)\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            return sess.run(out)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A function for training your RBM\n",
    "#Keep k at 1 for a traditional RBM. \n",
    "def train_cd_k(k, input_size, output_size, trX, teX, learning_rate, batch_size):\n",
    "    cdk_train_input = trX #Training data\n",
    "    cdk_test_input = teX  #Testing data\n",
    "    cdk_train_hidden = None #Variable to store hidden layer for training data\n",
    "    cdk_test_hidden = None  #Variable to store hidden layer for testing data\n",
    "    \n",
    "    rbm = RBM(input_size, output_size, learning_rate, batch_size)\n",
    "    \n",
    "    #Loop for contrastive divergence. \n",
    "    for i in range(k):\n",
    "        print('CD: {}'.format(int(i+1)))\n",
    "        rbm.train(cdk_train_input, cdk_test_input) #Using reconstruction as input layer for CD\n",
    "        cdk_train_input = rbm.new_input_layer\n",
    "        cdk_train_hidden = rbm.rbm_output(cdk_train_input)\n",
    "        cdk_test_hidden = rbm.rbm_output(cdk_test_input)\n",
    "        \n",
    "    return [cdk_train_hidden, cdk_test_hidden, cdk_train_input]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "    temp = train_cd_k(1,784,650,trX,teX,.001,32)\n",
    "    lsvc_RBM = LinearSVC()\n",
    "    lsvc_RBM.fit(temp[cdk_train_hidden], trY)\n",
    "    lsvc_RBM.score(temp[cdk_test_hidden], teY)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
